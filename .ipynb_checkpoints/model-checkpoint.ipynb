{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6fe342d3c0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import array\n",
    "from random import shuffle\n",
    "\n",
    "data1 = load('apple.npy')\n",
    "data2 = load('banana.npy')\n",
    "data3 = load('bird.npy')\n",
    "data4 = load('fork.npy')\n",
    "data5 = load('coffee cup.npy')\n",
    "data6 = load('baseball bat.npy')\n",
    "data7 = load('fish.npy')\n",
    "data8 = load('lollipop.npy')\n",
    "data9 = load('mushroom.npy')\n",
    "data0 = load('cloud.npy')\n",
    "data_array = [data1[0:40000], data2[0:40000],data3[0:40000], data4[0:40000],data5[0:40000], data6[0:40000],data7[0:40000], data8[0:40000],data9[0:40000], data0[0:40000]]\n",
    "data_array1 = [data1[40000:45000], data2[40000:45000],data3[40000:45000], data4[40000:45000],data5[40000:45000], data6[40000:45000],data7[40000:45000], data8[40000:45000],data9[40000:45000], data0[40000:45000]]\n",
    "train_input = np.concatenate((data1[0:40000], data2[0:40000],data3[0:40000], data4[0:40000],data5[0:40000], data6[0:40000],data7[0:40000], data8[0:40000],data9[0:40000], data0[0:40000]),axis=0)\n",
    "test_input = np.concatenate((data1[40000:45000], data2[40000:45000],data3[40000:45000], data4[40000:45000],data5[40000:45000], data6[40000:45000],data7[40000:45000], data8[40000:45000],data9[40000:45000], data0[40000:45000]),axis=0)\n",
    "\n",
    "train_folder_list = array(['apple','banana','bird', 'fork', 'coffee cup', 'baseball bat', 'fish', 'lollipop', 'mushroom', 'cloud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_label = []\n",
    " \n",
    "label_encoder = LabelEncoder()  # LabelEncoder Class 호출\n",
    "integer_encoded = label_encoder.fit_transform(train_folder_list)\n",
    "onehot_encoder = OneHotEncoder(sparse=False) \n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "\n",
    "for index in range(len(train_folder_list)):\n",
    "    for i in data_array[0]:\n",
    "        train_label.append([np.array(onehot_encoded[index])])\n",
    "        \n",
    "tmp1= [[x, y] for x, y in zip(train_input, train_label)]\n",
    "shuffle(tmp1)\n",
    "train_input = [arr[0] for arr in tmp1]\n",
    "train_label = [arr[1] for arr in tmp1]\n",
    "\n",
    "train_input = np.reshape(train_input, (-1, 784))\n",
    "train_label = np.reshape(train_label, (-1, 10))\n",
    "train_input = np.array(train_input).astype(np.float32)\n",
    "train_label = np.array(train_label).astype(np.float32)\n",
    "np.save(\"train_data.npy\", train_input)\n",
    "np.save(\"train_label.npy\", train_label)\n",
    "print(onehot_encoded)\n",
    "\n",
    "test_folder_list = array(['apple','banana','bird', 'fork', 'coffee cup', 'baseball bat', 'fish', 'lollipop', 'mushroom', 'cloud'])\n",
    " \n",
    "\n",
    "test_label = []\n",
    " \n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(test_folder_list)\n",
    " \n",
    "onehot_encoder = OneHotEncoder(sparse=False) \n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    " \n",
    "for index in range(len(test_folder_list)):\n",
    "    for img in data_array1[0]:\n",
    "        test_label.append([np.array(onehot_encoded[index])])\n",
    "\n",
    "tmp2= [[x, y] for x, y in zip(test_input, test_label)]\n",
    "shuffle(tmp2)\n",
    "test_input = [arr[0] for arr in tmp2]\n",
    "test_label = [arr[1] for arr in tmp2]\n",
    "test_input = np.reshape(test_input, (-1, 784))\n",
    "test_label = np.reshape(test_label, (-1, 10))\n",
    "test_input = np.array(test_input).astype(np.float32)\n",
    "test_label = np.array(test_label).astype(np.float32) \n",
    "np.save(\"test_input.npy\",test_input)\n",
    "np.save(\"test_label.npy\",test_label)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3cfbe38c9cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'apple.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'banana.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdata3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bird.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdata4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fork.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 440\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "learning_rate = 0.001\n",
    " \n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    " \n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    " \n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "W3 = tf.get_variable(\"W41\", shape=[7 * 7 * 64, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "test = tf.nn.softmax(logits)\n",
    " \n",
    "# define cost/loss &amp;amp;amp; optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    " \n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    " \n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver() \n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(len(train_input) / batch_size)\n",
    " \n",
    "    for i in range(total_batch):\n",
    "        start = ((i + 1) * batch_size) - batch_size\n",
    "        end = ((i + 1) * batch_size)\n",
    "        batch_xs = train_input[start:end]\n",
    "        batch_ys = train_label[start:end]\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    " \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    " \n",
    "print('Learning Finished!')\n",
    " \n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: test_input, Y: test_label}))\n",
    "\n",
    "saver.save(sess, 'cnn_session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10, Loss: 5.799116, Accuracy: 0.577080\n",
      "Step: 20, Loss: 4.251463, Accuracy: 0.643540\n",
      "Step: 30, Loss: 3.678324, Accuracy: 0.699480\n",
      "Step: 40, Loss: 3.387103, Accuracy: 0.732460\n",
      "Step: 50, Loss: 3.149220, Accuracy: 0.770480\n",
      "Step: 60, Loss: 3.017643, Accuracy: 0.777640\n",
      "Step: 70, Loss: 2.809717, Accuracy: 0.795180\n",
      "Step: 80, Loss: 2.782484, Accuracy: 0.796480\n",
      "Step: 90, Loss: 2.696849, Accuracy: 0.813060\n",
      "Step: 100, Loss: 2.665622, Accuracy: 0.803780\n",
      "Step: 110, Loss: 2.631832, Accuracy: 0.807600\n",
      "Step: 120, Loss: 2.510389, Accuracy: 0.819080\n",
      "Step: 130, Loss: 2.428223, Accuracy: 0.825180\n",
      "Step: 140, Loss: 2.358148, Accuracy: 0.835520\n",
      "Step: 150, Loss: 2.369202, Accuracy: 0.828240\n",
      "Step: 160, Loss: 2.250776, Accuracy: 0.839740\n",
      "Step: 170, Loss: 2.265629, Accuracy: 0.836700\n",
      "Step: 180, Loss: 2.299141, Accuracy: 0.836860\n",
      "Step: 190, Loss: 2.184628, Accuracy: 0.840880\n",
      "Step: 200, Loss: 2.189196, Accuracy: 0.842060\n",
      "Step: 210, Loss: 2.186247, Accuracy: 0.839140\n",
      "Step: 220, Loss: 2.162552, Accuracy: 0.838700\n",
      "Step: 230, Loss: 2.030898, Accuracy: 0.858540\n",
      "Step: 240, Loss: 2.008492, Accuracy: 0.856360\n",
      "Step: 250, Loss: 2.029766, Accuracy: 0.857400\n",
      "Step: 260, Loss: 1.909935, Accuracy: 0.864680\n",
      "Step: 270, Loss: 1.895871, Accuracy: 0.867640\n",
      "Step: 280, Loss: 1.867297, Accuracy: 0.869120\n",
      "Step: 290, Loss: 1.874275, Accuracy: 0.867660\n",
      "Step: 300, Loss: 1.849424, Accuracy: 0.870260\n",
      "Step: 310, Loss: 1.794391, Accuracy: 0.873280\n",
      "Step: 320, Loss: 1.793387, Accuracy: 0.872820\n",
      "Step: 330, Loss: 1.797565, Accuracy: 0.871620\n",
      "Step: 340, Loss: 1.727611, Accuracy: 0.880680\n",
      "Step: 350, Loss: 1.736565, Accuracy: 0.874540\n",
      "Step: 360, Loss: 1.704935, Accuracy: 0.880620\n",
      "Step: 370, Loss: 1.701417, Accuracy: 0.879800\n",
      "Step: 380, Loss: 1.686988, Accuracy: 0.880320\n",
      "Step: 390, Loss: 1.685689, Accuracy: 0.880520\n",
      "Step: 400, Loss: 1.668216, Accuracy: 0.883040\n",
      "Step: 410, Loss: 1.675970, Accuracy: 0.885000\n",
      "Step: 420, Loss: 1.653585, Accuracy: 0.886180\n",
      "Step: 430, Loss: 1.650656, Accuracy: 0.882760\n",
      "Step: 440, Loss: 1.658393, Accuracy: 0.882180\n",
      "Step: 450, Loss: 1.621956, Accuracy: 0.887520\n",
      "Step: 460, Loss: 1.684229, Accuracy: 0.878200\n",
      "Step: 470, Loss: 1.615396, Accuracy: 0.887560\n",
      "Step: 480, Loss: 1.606565, Accuracy: 0.886260\n",
      "Step: 490, Loss: 1.597986, Accuracy: 0.886560\n",
      "Step: 500, Loss: 1.564264, Accuracy: 0.889240\n",
      "Step: 510, Loss: 1.584576, Accuracy: 0.889100\n",
      "Step: 520, Loss: 1.574632, Accuracy: 0.888000\n",
      "Step: 530, Loss: 1.575589, Accuracy: 0.889100\n",
      "Step: 540, Loss: 1.556153, Accuracy: 0.891240\n",
      "Step: 550, Loss: 1.527071, Accuracy: 0.892340\n",
      "Step: 560, Loss: 1.554723, Accuracy: 0.888520\n",
      "Step: 570, Loss: 1.549657, Accuracy: 0.888820\n",
      "Step: 580, Loss: 1.534386, Accuracy: 0.891260\n",
      "Step: 590, Loss: 1.500362, Accuracy: 0.893600\n",
      "Step: 600, Loss: 1.629606, Accuracy: 0.879920\n",
      "Step: 610, Loss: 1.573620, Accuracy: 0.890860\n",
      "Step: 620, Loss: 1.598981, Accuracy: 0.881540\n",
      "Step: 630, Loss: 1.497330, Accuracy: 0.894880\n",
      "Step: 640, Loss: 1.462395, Accuracy: 0.898400\n",
      "Step: 650, Loss: 1.454711, Accuracy: 0.896700\n",
      "Step: 660, Loss: 1.481778, Accuracy: 0.897960\n",
      "Step: 670, Loss: 1.467748, Accuracy: 0.897660\n",
      "Step: 680, Loss: 1.433199, Accuracy: 0.899100\n",
      "Step: 690, Loss: 1.460645, Accuracy: 0.896760\n",
      "Step: 700, Loss: 1.467046, Accuracy: 0.895880\n",
      "Step: 710, Loss: 1.465639, Accuracy: 0.898080\n",
      "Step: 720, Loss: 1.437133, Accuracy: 0.897540\n",
      "Step: 730, Loss: 1.418631, Accuracy: 0.900440\n",
      "Step: 740, Loss: 1.449260, Accuracy: 0.898780\n",
      "Step: 750, Loss: 1.396491, Accuracy: 0.901780\n",
      "Step: 760, Loss: 1.449201, Accuracy: 0.898760\n",
      "Step: 770, Loss: 1.401908, Accuracy: 0.902140\n",
      "Step: 780, Loss: 1.412923, Accuracy: 0.900820\n",
      "Step: 790, Loss: 1.407631, Accuracy: 0.900280\n",
      "Step: 800, Loss: 1.408081, Accuracy: 0.898680\n",
      "Step: 810, Loss: 1.420869, Accuracy: 0.901020\n",
      "Step: 820, Loss: 1.400131, Accuracy: 0.899920\n",
      "Step: 830, Loss: 1.347649, Accuracy: 0.905980\n",
      "Step: 840, Loss: 1.356945, Accuracy: 0.903940\n",
      "Step: 850, Loss: 1.359610, Accuracy: 0.904040\n",
      "Step: 860, Loss: 1.389339, Accuracy: 0.900940\n",
      "Step: 870, Loss: 1.377012, Accuracy: 0.902680\n",
      "Step: 880, Loss: 1.366636, Accuracy: 0.904420\n",
      "Step: 890, Loss: 1.380918, Accuracy: 0.902060\n",
      "Step: 900, Loss: 1.362482, Accuracy: 0.904320\n",
      "Step: 910, Loss: 1.376282, Accuracy: 0.902720\n",
      "Step: 920, Loss: 1.348321, Accuracy: 0.906360\n",
      "Step: 930, Loss: 1.341113, Accuracy: 0.905580\n",
      "Step: 940, Loss: 1.330412, Accuracy: 0.906000\n",
      "Step: 950, Loss: 1.379078, Accuracy: 0.902460\n",
      "Step: 960, Loss: 1.330727, Accuracy: 0.904980\n",
      "Step: 970, Loss: 1.311079, Accuracy: 0.908400\n",
      "Step: 980, Loss: 1.308838, Accuracy: 0.907940\n",
      "Step: 990, Loss: 1.315368, Accuracy: 0.907780\n",
      "Step: 1000, Loss: 1.324359, Accuracy: 0.904660\n",
      "Step: 1010, Loss: 1.334683, Accuracy: 0.906960\n",
      "Step: 1020, Loss: 1.290089, Accuracy: 0.908340\n",
      "Step: 1030, Loss: 1.296389, Accuracy: 0.907240\n",
      "Step: 1040, Loss: 1.313121, Accuracy: 0.907880\n",
      "Step: 1050, Loss: 1.301851, Accuracy: 0.909540\n",
      "Step: 1060, Loss: 1.298271, Accuracy: 0.908700\n",
      "Step: 1070, Loss: 1.293803, Accuracy: 0.908200\n",
      "Step: 1080, Loss: 1.295975, Accuracy: 0.909820\n",
      "Step: 1090, Loss: 1.301443, Accuracy: 0.909920\n",
      "Step: 1100, Loss: 1.284951, Accuracy: 0.909660\n",
      "Step: 1110, Loss: 1.296496, Accuracy: 0.908740\n",
      "Step: 1120, Loss: 1.267096, Accuracy: 0.910640\n",
      "Step: 1130, Loss: 1.297099, Accuracy: 0.906740\n",
      "Step: 1140, Loss: 1.311285, Accuracy: 0.906360\n",
      "Step: 1150, Loss: 1.324433, Accuracy: 0.907940\n",
      "Step: 1160, Loss: 1.281706, Accuracy: 0.908800\n",
      "Step: 1170, Loss: 1.284486, Accuracy: 0.909020\n",
      "Step: 1180, Loss: 1.296567, Accuracy: 0.907120\n",
      "Step: 1190, Loss: 1.273694, Accuracy: 0.909760\n",
      "Step: 1200, Loss: 1.249315, Accuracy: 0.912000\n",
      "Step: 1210, Loss: 1.246179, Accuracy: 0.912360\n",
      "Step: 1220, Loss: 1.292126, Accuracy: 0.908440\n",
      "Step: 1230, Loss: 1.273534, Accuracy: 0.909720\n",
      "Step: 1240, Loss: 1.280034, Accuracy: 0.910720\n",
      "Step: 1250, Loss: 1.309209, Accuracy: 0.905460\n",
      "Step: 1260, Loss: 1.317004, Accuracy: 0.906880\n",
      "Step: 1270, Loss: 1.244873, Accuracy: 0.910940\n",
      "Step: 1280, Loss: 1.251794, Accuracy: 0.910580\n",
      "Step: 1290, Loss: 1.274988, Accuracy: 0.910240\n",
      "Step: 1300, Loss: 1.257361, Accuracy: 0.910660\n",
      "Step: 1310, Loss: 1.252867, Accuracy: 0.912180\n",
      "Step: 1320, Loss: 1.245425, Accuracy: 0.911920\n",
      "Step: 1330, Loss: 1.276186, Accuracy: 0.908780\n",
      "Step: 1340, Loss: 1.244230, Accuracy: 0.912040\n",
      "Step: 1350, Loss: 1.233603, Accuracy: 0.912400\n",
      "Step: 1360, Loss: 1.235140, Accuracy: 0.911800\n",
      "Step: 1370, Loss: 1.241068, Accuracy: 0.912380\n",
      "Step: 1380, Loss: 1.260228, Accuracy: 0.909180\n",
      "Step: 1390, Loss: 1.229747, Accuracy: 0.913300\n",
      "Step: 1400, Loss: 1.221760, Accuracy: 0.914520\n",
      "Step: 1410, Loss: 1.250942, Accuracy: 0.910220\n",
      "Step: 1420, Loss: 1.243496, Accuracy: 0.912120\n",
      "Step: 1430, Loss: 1.223896, Accuracy: 0.911940\n",
      "Step: 1440, Loss: 1.211392, Accuracy: 0.911940\n",
      "Step: 1450, Loss: 1.225415, Accuracy: 0.911380\n",
      "Step: 1460, Loss: 1.236278, Accuracy: 0.911220\n",
      "Step: 1470, Loss: 1.277549, Accuracy: 0.906700\n",
      "Step: 1480, Loss: 1.215024, Accuracy: 0.913440\n",
      "Step: 1490, Loss: 1.230046, Accuracy: 0.911920\n",
      "Step: 1500, Loss: 1.225397, Accuracy: 0.911880\n",
      "Step: 1510, Loss: 1.217461, Accuracy: 0.913660\n",
      "Step: 1520, Loss: 1.207199, Accuracy: 0.914320\n",
      "Step: 1530, Loss: 1.208644, Accuracy: 0.914660\n",
      "Step: 1540, Loss: 1.208824, Accuracy: 0.913700\n",
      "Step: 1550, Loss: 1.199530, Accuracy: 0.914220\n",
      "Step: 1560, Loss: 1.202797, Accuracy: 0.916140\n",
      "Step: 1570, Loss: 1.219607, Accuracy: 0.912380\n",
      "Step: 1580, Loss: 1.226768, Accuracy: 0.913600\n",
      "Step: 1590, Loss: 1.197417, Accuracy: 0.915360\n",
      "Step: 1600, Loss: 1.203383, Accuracy: 0.913440\n",
      "Step: 1610, Loss: 1.207505, Accuracy: 0.914700\n",
      "Step: 1620, Loss: 1.185526, Accuracy: 0.915840\n",
      "Step: 1630, Loss: 1.194119, Accuracy: 0.914040\n",
      "Step: 1640, Loss: 1.184663, Accuracy: 0.915700\n",
      "Step: 1650, Loss: 1.240624, Accuracy: 0.910160\n",
      "Step: 1660, Loss: 1.197523, Accuracy: 0.915080\n",
      "Step: 1670, Loss: 1.190835, Accuracy: 0.913860\n",
      "Step: 1680, Loss: 1.194999, Accuracy: 0.915680\n",
      "Step: 1690, Loss: 1.182644, Accuracy: 0.914320\n",
      "Step: 1700, Loss: 1.208841, Accuracy: 0.913540\n",
      "Step: 1710, Loss: 1.169894, Accuracy: 0.916400\n",
      "Step: 1720, Loss: 1.175275, Accuracy: 0.915480\n",
      "Step: 1730, Loss: 1.175333, Accuracy: 0.917960\n",
      "Step: 1740, Loss: 1.181972, Accuracy: 0.916920\n",
      "Step: 1750, Loss: 1.203060, Accuracy: 0.912940\n",
      "Step: 1760, Loss: 1.220343, Accuracy: 0.912060\n",
      "Step: 1770, Loss: 1.174397, Accuracy: 0.918800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1780, Loss: 1.164406, Accuracy: 0.917660\n",
      "Step: 1790, Loss: 1.176173, Accuracy: 0.917720\n",
      "Step: 1800, Loss: 1.162149, Accuracy: 0.918140\n",
      "Step: 1810, Loss: 1.173681, Accuracy: 0.916040\n",
      "Step: 1820, Loss: 1.173290, Accuracy: 0.917200\n",
      "Step: 1830, Loss: 1.160825, Accuracy: 0.917960\n",
      "Step: 1840, Loss: 1.167554, Accuracy: 0.916400\n",
      "Step: 1850, Loss: 1.177680, Accuracy: 0.915980\n",
      "Step: 1860, Loss: 1.182342, Accuracy: 0.915280\n",
      "Step: 1870, Loss: 1.160890, Accuracy: 0.918460\n",
      "Step: 1880, Loss: 1.141534, Accuracy: 0.919280\n",
      "Step: 1890, Loss: 1.145887, Accuracy: 0.918960\n",
      "Step: 1900, Loss: 1.178781, Accuracy: 0.916520\n",
      "Step: 1910, Loss: 1.222500, Accuracy: 0.913140\n",
      "Step: 1920, Loss: 1.197716, Accuracy: 0.913500\n",
      "Step: 1930, Loss: 1.152274, Accuracy: 0.917680\n",
      "Step: 1940, Loss: 1.165107, Accuracy: 0.916980\n",
      "Step: 1950, Loss: 1.167565, Accuracy: 0.917080\n",
      "Step: 1960, Loss: 1.156786, Accuracy: 0.919260\n",
      "Step: 1970, Loss: 1.190076, Accuracy: 0.915160\n",
      "Step: 1980, Loss: 1.139419, Accuracy: 0.919760\n",
      "Step: 1990, Loss: 1.133443, Accuracy: 0.919860\n",
      "Step: 2000, Loss: 1.135984, Accuracy: 0.919700\n",
      "Step: 2010, Loss: 1.121691, Accuracy: 0.920340\n",
      "Step: 2020, Loss: 1.131203, Accuracy: 0.919960\n",
      "Step: 2030, Loss: 1.159346, Accuracy: 0.917820\n",
      "Step: 2040, Loss: 1.140562, Accuracy: 0.919420\n",
      "Step: 2050, Loss: 1.133464, Accuracy: 0.920060\n",
      "Step: 2060, Loss: 1.138066, Accuracy: 0.919600\n",
      "Step: 2070, Loss: 1.166128, Accuracy: 0.917660\n",
      "Step: 2080, Loss: 1.174813, Accuracy: 0.917100\n",
      "Step: 2090, Loss: 1.167476, Accuracy: 0.918220\n",
      "Step: 2100, Loss: 1.134053, Accuracy: 0.920740\n",
      "Step: 2110, Loss: 1.149530, Accuracy: 0.918320\n",
      "Step: 2120, Loss: 1.125572, Accuracy: 0.919860\n",
      "Step: 2130, Loss: 1.146980, Accuracy: 0.917760\n",
      "Step: 2140, Loss: 1.158993, Accuracy: 0.919300\n",
      "Step: 2150, Loss: 1.167191, Accuracy: 0.916940\n",
      "Step: 2160, Loss: 1.129667, Accuracy: 0.919520\n",
      "Step: 2170, Loss: 1.110941, Accuracy: 0.921460\n",
      "Step: 2180, Loss: 1.130360, Accuracy: 0.920120\n",
      "Step: 2190, Loss: 1.108216, Accuracy: 0.921320\n",
      "Step: 2200, Loss: 1.158278, Accuracy: 0.917720\n",
      "Step: 2210, Loss: 1.146516, Accuracy: 0.917580\n",
      "Step: 2220, Loss: 1.138719, Accuracy: 0.918840\n",
      "Step: 2230, Loss: 1.151601, Accuracy: 0.918900\n",
      "Step: 2240, Loss: 1.122983, Accuracy: 0.918880\n",
      "Step: 2250, Loss: 1.144918, Accuracy: 0.919160\n",
      "Step: 2260, Loss: 1.117438, Accuracy: 0.919900\n",
      "Step: 2270, Loss: 1.125262, Accuracy: 0.918660\n",
      "Step: 2280, Loss: 1.144096, Accuracy: 0.918280\n",
      "Step: 2290, Loss: 1.136447, Accuracy: 0.918300\n",
      "Step: 2300, Loss: 1.113360, Accuracy: 0.920600\n",
      "Step: 2310, Loss: 1.119696, Accuracy: 0.920600\n",
      "Step: 2320, Loss: 1.149742, Accuracy: 0.917820\n",
      "Step: 2330, Loss: 1.127517, Accuracy: 0.918480\n",
      "Step: 2340, Loss: 1.106004, Accuracy: 0.920540\n",
      "Step: 2350, Loss: 1.116886, Accuracy: 0.920940\n",
      "Step: 2360, Loss: 1.133458, Accuracy: 0.917520\n",
      "Step: 2370, Loss: 1.122381, Accuracy: 0.919340\n",
      "Step: 2380, Loss: 1.118090, Accuracy: 0.920340\n",
      "Step: 2390, Loss: 1.104750, Accuracy: 0.921540\n",
      "Step: 2400, Loss: 1.099705, Accuracy: 0.921420\n",
      "Step: 2410, Loss: 1.103333, Accuracy: 0.921060\n",
      "Step: 2420, Loss: 1.114558, Accuracy: 0.920640\n",
      "Step: 2430, Loss: 1.142724, Accuracy: 0.919020\n",
      "Step: 2440, Loss: 1.125434, Accuracy: 0.920040\n",
      "Step: 2450, Loss: 1.091140, Accuracy: 0.922340\n",
      "Step: 2460, Loss: 1.096726, Accuracy: 0.921580\n",
      "Step: 2470, Loss: 1.099213, Accuracy: 0.922020\n",
      "Step: 2480, Loss: 1.084464, Accuracy: 0.922520\n",
      "Step: 2490, Loss: 1.088776, Accuracy: 0.922820\n",
      "Step: 2500, Loss: 1.136146, Accuracy: 0.919240\n",
      "Step: 2510, Loss: 1.110926, Accuracy: 0.921420\n",
      "Step: 2520, Loss: 1.078665, Accuracy: 0.923220\n",
      "Step: 2530, Loss: 1.098317, Accuracy: 0.921680\n",
      "Step: 2540, Loss: 1.091007, Accuracy: 0.922700\n",
      "Step: 2550, Loss: 1.097931, Accuracy: 0.922640\n",
      "Step: 2560, Loss: 1.099594, Accuracy: 0.922560\n",
      "Step: 2570, Loss: 1.105148, Accuracy: 0.920700\n",
      "Step: 2580, Loss: 1.096900, Accuracy: 0.921920\n",
      "Step: 2590, Loss: 1.068078, Accuracy: 0.924580\n",
      "Step: 2600, Loss: 1.102211, Accuracy: 0.921280\n",
      "Step: 2610, Loss: 1.077386, Accuracy: 0.923580\n",
      "Step: 2620, Loss: 1.089173, Accuracy: 0.922000\n",
      "Step: 2630, Loss: 1.069636, Accuracy: 0.923960\n",
      "Step: 2640, Loss: 1.089318, Accuracy: 0.923180\n",
      "Step: 2650, Loss: 1.072241, Accuracy: 0.924620\n",
      "Step: 2660, Loss: 1.083623, Accuracy: 0.923560\n",
      "Step: 2670, Loss: 1.089348, Accuracy: 0.922880\n",
      "Step: 2680, Loss: 1.086303, Accuracy: 0.922740\n",
      "Step: 2690, Loss: 1.079743, Accuracy: 0.924020\n",
      "Step: 2700, Loss: 1.073375, Accuracy: 0.923460\n",
      "Step: 2710, Loss: 1.087648, Accuracy: 0.922240\n",
      "Step: 2720, Loss: 1.081858, Accuracy: 0.921480\n",
      "Step: 2730, Loss: 1.067488, Accuracy: 0.922920\n",
      "Step: 2740, Loss: 1.072857, Accuracy: 0.922780\n",
      "Step: 2750, Loss: 1.089477, Accuracy: 0.921400\n",
      "Step: 2760, Loss: 1.079720, Accuracy: 0.922040\n",
      "Step: 2770, Loss: 1.075109, Accuracy: 0.922880\n",
      "Step: 2780, Loss: 1.079665, Accuracy: 0.922880\n",
      "Step: 2790, Loss: 1.077842, Accuracy: 0.922060\n",
      "Step: 2800, Loss: 1.070578, Accuracy: 0.923380\n",
      "Step: 2810, Loss: 1.086827, Accuracy: 0.921800\n",
      "Step: 2820, Loss: 1.069778, Accuracy: 0.923180\n",
      "Step: 2830, Loss: 1.072207, Accuracy: 0.923180\n",
      "Step: 2840, Loss: 1.069066, Accuracy: 0.923760\n",
      "Step: 2850, Loss: 1.056418, Accuracy: 0.924480\n",
      "Step: 2860, Loss: 1.077620, Accuracy: 0.922800\n",
      "Step: 2870, Loss: 1.057691, Accuracy: 0.925560\n",
      "Step: 2880, Loss: 1.073663, Accuracy: 0.923280\n",
      "Step: 2890, Loss: 1.062493, Accuracy: 0.923780\n",
      "Step: 2900, Loss: 1.069796, Accuracy: 0.923220\n",
      "Step: 2910, Loss: 1.050004, Accuracy: 0.925160\n",
      "Step: 2920, Loss: 1.077958, Accuracy: 0.921780\n",
      "Step: 2930, Loss: 1.057523, Accuracy: 0.924080\n",
      "Step: 2940, Loss: 1.083482, Accuracy: 0.922140\n",
      "Step: 2950, Loss: 1.065361, Accuracy: 0.923620\n",
      "Step: 2960, Loss: 1.107925, Accuracy: 0.919300\n",
      "Step: 2970, Loss: 1.039905, Accuracy: 0.925320\n",
      "Step: 2980, Loss: 1.059022, Accuracy: 0.924360\n",
      "Step: 2990, Loss: 1.078385, Accuracy: 0.921740\n",
      "Step: 3000, Loss: 1.044859, Accuracy: 0.924800\n",
      "Step: 3010, Loss: 1.052999, Accuracy: 0.924760\n",
      "Step: 3020, Loss: 1.070442, Accuracy: 0.923160\n",
      "Step: 3030, Loss: 1.052635, Accuracy: 0.924420\n",
      "Step: 3040, Loss: 1.070940, Accuracy: 0.922860\n",
      "Step: 3050, Loss: 1.082327, Accuracy: 0.922860\n",
      "Step: 3060, Loss: 1.094056, Accuracy: 0.919700\n",
      "Step: 3070, Loss: 1.081726, Accuracy: 0.922180\n",
      "Step: 3080, Loss: 1.065716, Accuracy: 0.923160\n",
      "Step: 3090, Loss: 1.048960, Accuracy: 0.924300\n",
      "Step: 3100, Loss: 1.054649, Accuracy: 0.924260\n",
      "Step: 3110, Loss: 1.038463, Accuracy: 0.925120\n",
      "Step: 3120, Loss: 1.046939, Accuracy: 0.924400\n",
      "Step: 3130, Loss: 1.063618, Accuracy: 0.923200\n",
      "Step: 3140, Loss: 1.060728, Accuracy: 0.924000\n",
      "Step: 3150, Loss: 1.050806, Accuracy: 0.923800\n",
      "Step: 3160, Loss: 1.042678, Accuracy: 0.925560\n",
      "Step: 3170, Loss: 1.029511, Accuracy: 0.925540\n",
      "Step: 3180, Loss: 1.048822, Accuracy: 0.925220\n",
      "Step: 3190, Loss: 1.039524, Accuracy: 0.925640\n",
      "Step: 3200, Loss: 1.046375, Accuracy: 0.924760\n",
      "Step: 3210, Loss: 1.047350, Accuracy: 0.925380\n",
      "Step: 3220, Loss: 1.049031, Accuracy: 0.923820\n",
      "Step: 3230, Loss: 1.035310, Accuracy: 0.925740\n",
      "Step: 3240, Loss: 1.050242, Accuracy: 0.924340\n",
      "Step: 3250, Loss: 1.046882, Accuracy: 0.925020\n",
      "Step: 3260, Loss: 1.041498, Accuracy: 0.924440\n",
      "Step: 3270, Loss: 1.056369, Accuracy: 0.923600\n",
      "Step: 3280, Loss: 1.062377, Accuracy: 0.923280\n",
      "Step: 3290, Loss: 1.061751, Accuracy: 0.923580\n",
      "Step: 3300, Loss: 1.073134, Accuracy: 0.922360\n",
      "Step: 3310, Loss: 1.029496, Accuracy: 0.926160\n",
      "Step: 3320, Loss: 1.044534, Accuracy: 0.924660\n",
      "Step: 3330, Loss: 1.027297, Accuracy: 0.926440\n",
      "Step: 3340, Loss: 1.025627, Accuracy: 0.927260\n",
      "Step: 3350, Loss: 1.032845, Accuracy: 0.925520\n",
      "Step: 3360, Loss: 1.046028, Accuracy: 0.925940\n",
      "Step: 3370, Loss: 1.036780, Accuracy: 0.925740\n",
      "Step: 3380, Loss: 1.042349, Accuracy: 0.925000\n",
      "Step: 3390, Loss: 1.019830, Accuracy: 0.927280\n",
      "Step: 3400, Loss: 1.033408, Accuracy: 0.926360\n",
      "Step: 3410, Loss: 1.026700, Accuracy: 0.927100\n",
      "Step: 3420, Loss: 1.021841, Accuracy: 0.926620\n",
      "Step: 3430, Loss: 1.041648, Accuracy: 0.924840\n",
      "Step: 3440, Loss: 1.014986, Accuracy: 0.927240\n",
      "Step: 3450, Loss: 1.049260, Accuracy: 0.925600\n",
      "Step: 3460, Loss: 1.042336, Accuracy: 0.925540\n",
      "Step: 3470, Loss: 1.047361, Accuracy: 0.924800\n",
      "Step: 3480, Loss: 1.036100, Accuracy: 0.925840\n",
      "Step: 3490, Loss: 1.030916, Accuracy: 0.926340\n",
      "Step: 3500, Loss: 1.026880, Accuracy: 0.926020\n",
      "Step: 3510, Loss: 1.030715, Accuracy: 0.926200\n",
      "Step: 3520, Loss: 1.026166, Accuracy: 0.925820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3530, Loss: 1.033696, Accuracy: 0.926040\n",
      "Step: 3540, Loss: 1.043940, Accuracy: 0.923960\n",
      "Step: 3550, Loss: 1.016564, Accuracy: 0.927820\n",
      "Step: 3560, Loss: 1.037163, Accuracy: 0.925680\n",
      "Step: 3570, Loss: 1.070161, Accuracy: 0.923660\n",
      "Step: 3580, Loss: 1.014598, Accuracy: 0.927540\n",
      "Step: 3590, Loss: 1.027091, Accuracy: 0.925520\n",
      "Step: 3600, Loss: 1.035187, Accuracy: 0.925100\n",
      "Step: 3610, Loss: 1.026691, Accuracy: 0.925240\n",
      "Step: 3620, Loss: 1.010808, Accuracy: 0.927180\n",
      "Step: 3630, Loss: 1.039764, Accuracy: 0.924900\n",
      "Step: 3640, Loss: 1.003173, Accuracy: 0.928000\n",
      "Step: 3650, Loss: 1.023710, Accuracy: 0.926420\n",
      "Step: 3660, Loss: 1.015429, Accuracy: 0.926740\n",
      "Step: 3670, Loss: 1.044429, Accuracy: 0.923580\n",
      "Step: 3680, Loss: 1.027664, Accuracy: 0.925500\n",
      "Step: 3690, Loss: 1.004298, Accuracy: 0.927400\n",
      "Step: 3700, Loss: 1.009856, Accuracy: 0.927880\n",
      "Step: 3710, Loss: 1.002110, Accuracy: 0.928060\n",
      "Step: 3720, Loss: 1.036634, Accuracy: 0.925260\n",
      "Step: 3730, Loss: 1.036267, Accuracy: 0.925820\n",
      "Step: 3740, Loss: 1.030300, Accuracy: 0.926300\n",
      "Step: 3750, Loss: 1.024421, Accuracy: 0.926640\n",
      "Step: 3760, Loss: 1.040782, Accuracy: 0.925940\n",
      "Step: 3770, Loss: 1.009051, Accuracy: 0.927220\n",
      "Step: 3780, Loss: 1.025178, Accuracy: 0.926480\n",
      "Step: 3790, Loss: 1.009939, Accuracy: 0.927160\n",
      "Step: 3800, Loss: 1.006671, Accuracy: 0.926940\n",
      "Step: 3810, Loss: 0.999244, Accuracy: 0.927880\n",
      "Step: 3820, Loss: 0.992198, Accuracy: 0.929020\n",
      "Step: 3830, Loss: 1.011545, Accuracy: 0.927100\n",
      "Step: 3840, Loss: 1.012825, Accuracy: 0.926960\n",
      "Step: 3850, Loss: 1.017092, Accuracy: 0.926900\n",
      "Step: 3860, Loss: 1.012556, Accuracy: 0.926600\n",
      "Step: 3870, Loss: 0.999121, Accuracy: 0.927420\n",
      "Step: 3880, Loss: 1.029730, Accuracy: 0.925340\n",
      "Step: 3890, Loss: 1.017089, Accuracy: 0.926780\n",
      "Step: 3900, Loss: 1.032604, Accuracy: 0.926040\n",
      "Step: 3910, Loss: 1.019765, Accuracy: 0.926420\n",
      "Step: 3920, Loss: 1.006585, Accuracy: 0.927260\n",
      "Step: 3930, Loss: 0.991440, Accuracy: 0.928480\n",
      "Step: 3940, Loss: 1.011437, Accuracy: 0.926640\n",
      "Step: 3950, Loss: 0.991478, Accuracy: 0.927440\n",
      "Step: 3960, Loss: 0.992354, Accuracy: 0.928060\n",
      "Step: 3970, Loss: 0.999015, Accuracy: 0.928240\n",
      "Step: 3980, Loss: 1.024073, Accuracy: 0.926200\n",
      "Step: 3990, Loss: 1.002424, Accuracy: 0.927920\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(20160704)\n",
    "tf.set_random_seed(20160704)\n",
    "\n",
    "num_filters1 = 32\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) \n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3,3,1,num_filters1],stddev=0.01))\n",
    "\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "h_conv1_cutoff = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1,2,2,1], strides = [1,2,2,1], padding='SAME')\n",
    "\n",
    "num_filters2 = 64\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([3,3, num_filters1, num_filters2], stddev=0.01))\n",
    "h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1,1,1,1], padding= 'SAME')\n",
    "\n",
    "b_conv2 = tf.Variable(tf.constant(0.1,shape=[num_filters2]))\n",
    "h_conv2_cutoff = tf.nn.relu(h_conv2 + b_conv2)\n",
    "\n",
    "h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*num_filters2])\n",
    "\n",
    "num_units1 = 7*7*num_filters2\n",
    "num_units2 = 1024\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([num_units1, num_units2]))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "hidden2 = tf.nn.relu(tf.matmul(h_pool2_flat, w2) +b2)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden2_drop = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "w0 = tf.Variable(tf.zeros([num_units2, 10]))\n",
    "b0 = tf.Variable(tf.zeros([10]))\n",
    "k = tf.matmul(hidden2_drop, w0) + b0\n",
    "p = tf.nn.softmax(k)\n",
    "\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=t,logits=k)) \n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf. train.Saver()\n",
    "i =0\n",
    "# for i in range(4000):\n",
    "#     batch_xs = train_input[100*i: 100*i + 100]\n",
    "#     batch_ts = train_label[100*i: 100*i + 100]\n",
    "#     sess.run(train_step, feed_dict={x:batch_xs, t:batch_ts, keep_prob:0.5})\n",
    "#     if i % 500 == 0:\n",
    "#         loss_vals, acc_vals = [], []\n",
    "#         for c in range(4):\n",
    "#             start = int(len(test_input)/4 * c)\n",
    "#             end = int(len(test_input)/4 * (c+1))\n",
    "#             loss_val, acc_val = sess.run([loss, accuracy], feed_dict={x:test_input[start:end], t:test_label[start:end], keep_prod:1.0})\n",
    "#             loss_vals.append(loss_val)\n",
    "#             acc_vals.append(acc_val)\n",
    "#         loss_val = np.sum(loss_vals)\n",
    "#         acc_val = np.mean(acc_vals)\n",
    "#         print('Step: %d, Loss: %f, Accuracy: %f' % (i,loss_val, acc_val))\n",
    "\n",
    "for _ in range(3999):\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    batch_xs = train_input[100*i: 100*i + 100]\n",
    "    batch_ts = train_label[100*i: 100*i + 100]\n",
    "\n",
    "    sess.run(train_step,feed_dict={x:batch_xs, t:batch_ts, keep_prob:0.5})\n",
    "    if i % 10 == 0:\n",
    "        loss_vals, acc_vals = [], []\n",
    "\n",
    "        for c in range(4):\n",
    "\n",
    "            start = int(len(test_input) / 4 * c)\n",
    "\n",
    "            end = int(len(test_input) / 4 * (c+1))\n",
    "\n",
    "            loss_val, acc_val = sess.run([loss, accuracy],feed_dict={x:test_input[start:end], t:test_label[start:end],keep_prob:1.0})\n",
    "\n",
    "            loss_vals.append(loss_val)\n",
    "\n",
    "            acc_vals.append(acc_val)\n",
    "\n",
    "        loss_val = np.sum(loss_vals)\n",
    "\n",
    "        acc_val = np.mean(acc_vals)\n",
    "\n",
    "        print ('Step: %d, Loss: %f, Accuracy: %f'\n",
    "\n",
    "               % (i, loss_val, acc_val))\n",
    "saver.save(sess, 'cnn_session1')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 180 42000 180\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input),len(test_input),len(train_label), len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100000,28,28,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node MklInputConversion/_24}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100000,28,28,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node MklInputConversion/_24}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ab491959c35b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    156\u001b[0m                            \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                            keep_prob:1.0})\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mloss_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tenpy\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100000,28,28,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node MklInputConversion/_24}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_filters1 = 32\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,num_filters1],\n",
    "\n",
    "                                          stddev=0.1))\n",
    "\n",
    "h_conv1 = tf.nn.conv2d(x_image, W_conv1,\n",
    "\n",
    "                       strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "\n",
    "h_conv1_cutoff = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "\n",
    "\n",
    "h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1,2,2,1],\n",
    "\n",
    "                         strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "# define second layer\n",
    "\n",
    "num_filters2 = 64\n",
    "\n",
    "\n",
    "\n",
    "W_conv2 = tf.Variable(\n",
    "\n",
    "            tf.truncated_normal([5,5,num_filters1,num_filters2],\n",
    "\n",
    "                                stddev=0.1))\n",
    "\n",
    "h_conv2 = tf.nn.conv2d(h_pool1, W_conv2,\n",
    "\n",
    "                       strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "\n",
    "h_conv2_cutoff = tf.nn.relu(h_conv2 + b_conv2)\n",
    "\n",
    "\n",
    "\n",
    "h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1,2,2,1],\n",
    "\n",
    "                         strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "# define fully connected layer\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*num_filters2])\n",
    "\n",
    "\n",
    "\n",
    "num_units1 = 7*7*num_filters2\n",
    "\n",
    "num_units2 = 1024\n",
    "\n",
    "\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([num_units1, num_units2]))\n",
    "\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "\n",
    "hidden2 = tf.nn.relu(tf.matmul(h_pool2_flat, w2) + b2)\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "hidden2_drop = tf.nn.dropout(hidden2, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "w0 = tf.Variable(tf.zeros([num_units2, 10]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "k = tf.matmul(hidden2_drop, w0) + b0\n",
    "\n",
    "p = tf.nn.softmax(k)\n",
    "\n",
    "\n",
    "\n",
    "#define loss (cost) function\n",
    "\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=t,logits=k)) \n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# prepare session\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "# start training\n",
    "\n",
    "i = 0\n",
    "batch_size = 40\n",
    "for _ in range(10000):\n",
    "\n",
    "    i += 1\n",
    "    start = ((i + 1) * batch_size) - batch_size\n",
    "    end = ((i + 1) * batch_size)\n",
    "    batch_xs, batch_ts = train_input[start:end],train_label[start:end]\n",
    "\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, t:batch_ts, keep_prob:0.5})\n",
    "\n",
    "    if i % 100 == 0:\n",
    "\n",
    "        loss_vals, acc_vals = [], []\n",
    "\n",
    "        for c in range(4):\n",
    "\n",
    "            start = len(train_input) / 4 * c\n",
    "\n",
    "            end = len(train_input) / 4 * (c+1)\n",
    "            \n",
    "            start, end = int(start), int(end)\n",
    "\n",
    "            loss_val, acc_val = sess.run([loss, accuracy],\n",
    "\n",
    "                feed_dict={x:train_input[start:end],\n",
    "\n",
    "                           t:train_label[start:end],\n",
    "\n",
    "                           keep_prob:1.0})\n",
    "\n",
    "            loss_vals.append(loss_val)\n",
    "\n",
    "            acc_vals.append(acc_val)\n",
    "\n",
    "        loss_val = np.sum(loss_vals)\n",
    "\n",
    "        acc_val = np.mean(acc_vals)\n",
    "\n",
    "        print ('Step: %d, Loss: %f, Accuracy: %f' % (i, loss_val, acc_val))\n",
    "\n",
    "\n",
    "\n",
    "saver.save(sess, 'cnn_session')\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
